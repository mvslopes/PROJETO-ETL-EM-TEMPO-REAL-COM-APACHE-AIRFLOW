{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT0qLqIYiCHGgyJG3qMFUq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Código Python para ETL- Extração, Transformação e Carregamento**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQVxJN92XFJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bsqRb0s2MVkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import csv\n",
        "import airflow\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.postgres_operator import PostgresOperator\n",
        "from airflow.utils.dates import days_ago\n",
        "\n",
        "# Argumentos\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Cria a DAG\n",
        "# https://crontab.guru/\n",
        "dag_log_solutions = DAG(dag_id = \"logsol\",\n",
        "                   default_args = default_args,\n",
        "                   schedule_interval = '0 0 * * *',\n",
        "                   dagrun_timeout = timedelta(minutes = 60),\n",
        "                   description = 'Job ETL de Carga no DW com Airflow',\n",
        "                   start_date = airflow.utils.dates.days_ago(1)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "##### Tabela de Clientes #####\n",
        "\n",
        "def func_carrega_dados_clientes(**kwargs):\n",
        "\n",
        "    # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_CLIENTE ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_CLIENTE.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "##### Tabela de Transportadoras #####\n",
        "\n",
        "def func_carrega_dados_transportadora(**kwargs):\n",
        "\n",
        "    # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_TRANSPORTADORA ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_transportadora_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_transportadora = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_transportadora',\n",
        "        python_callable = func_carrega_dados_transportadora,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_TRANSPORTADORA.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "##### Tabela de Depósitos #####\n",
        "\n",
        "def func_carrega_dados_deposito(**kwargs):\n",
        "\n",
        "    # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_DEPOSITO ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_deposito_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_deposito = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_deposito',\n",
        "        python_callable = func_carrega_dados_deposito,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_DEPOSITO.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "##### Tabela de Entregas #####\n",
        "\n",
        "def func_carrega_dados_entrega(**kwargs):\n",
        "\n",
        "  # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_ENTREGA ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_entrega_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_entrega = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_entrega',\n",
        "        python_callable = func_carrega_dados_entrega,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_ENTREGA.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "##### Tabela de Frete #####\n",
        "\n",
        "def func_carrega_dados_frete(**kwargs):\n",
        "\n",
        "     # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_FRETE ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_frete_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_frete = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_frete',\n",
        "        python_callable = func_carrega_dados_frete,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_FRETE.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Tabela de Tipos de Pagamentos #####\n",
        "\n",
        "def func_carrega_dados_pagamento(**kwargs):\n",
        "\n",
        "     # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_PAGAMENTO ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_pagamento_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_pagamento = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_pagamento',\n",
        "        python_callable = func_carrega_dados_pagamento,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_PAGAMENTO.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Tabela de Data #####\n",
        "\n",
        "def func_carrega_dados_data(**kwargs):\n",
        "\n",
        "     # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_DATA ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_data_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_data = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_data',\n",
        "        python_callable = func_carrega_dados_data,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_DATA.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "##### Tabela de Fatos #####\n",
        "\n",
        "def func_carrega_dados_fatos(**kwargs):\n",
        "\n",
        "    # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.TB_FATO ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_fatos_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_fatos = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_fatos',\n",
        "        python_callable = func_carrega_dados_fatos,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/TB_FATOS.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Tarefas para limpar as tabelas\n",
        "tarefa_trunca_tb_fato = PostgresOperator(task_id = 'tarefa_trunca_tb_fato', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.TB_FATO CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_cliente = PostgresOperator(task_id = 'tarefa_trunca_dim_cliente', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_CLIENTE CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_pagamento = PostgresOperator(task_id = 'tarefa_trunca_dim_pagamento', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_PAGAMENTO CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_frete = PostgresOperator(task_id = 'tarefa_trunca_dim_frete', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_FRETE CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_data = PostgresOperator(task_id = 'tarefa_trunca_dim_data', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_DATA CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_transportadora = PostgresOperator(task_id = 'tarefa_trunca_dim_transportadora', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_TRANSPORTADORA CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_entrega = PostgresOperator(task_id = 'tarefa_trunca_dim_entrega', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_ENTREGA CASCADE\", dag = dag_log_solutions)\n",
        "tarefa_trunca_dim_deposito = PostgresOperator(task_id = 'tarefa_trunca_dim_deposito', postgres_conn_id = 'LOGDW', sql = \"TRUNCATE TABLE varejo.DIM_DEPOSITO CASCADE\", dag = dag_log_solutions)\n",
        "\n",
        "# Upstream\n",
        "tarefa_trunca_tb_fato >> tarefa_trunca_dim_cliente >> tarefa_trunca_dim_pagamento >> tarefa_trunca_dim_frete >> tarefa_trunca_dim_data >> tarefa_trunca_dim_transportadora >> tarefa_trunca_dim_entrega >> tarefa_trunca_dim_deposito >> tarefa_carrega_dados_clientes >> tarefa_carrega_dados_transportadora >> tarefa_carrega_dados_deposito >> tarefa_carrega_dados_entrega >> tarefa_carrega_dados_frete >> tarefa_carrega_dados_pagamento >> tarefa_carrega_dados_data >> tarefa_carrega_dados_fatos\n",
        "\n",
        "# Bloco main\n",
        "if __name__ == \"__main__\":\n",
        "    dag_log_solutions.cli()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ-0Jar1MXLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}