{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdN18Foh7FaD0NRdPZIlUS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**SPRINT 02 - Desenvolvimento dos códigos ETL em Python**\n",
        "\n",
        "O principal objetivo da Sprint 02 foi avançar na implementação do **Data Warehouse**, focando especificamente no desenvolvimento do processo de **ETL (Extração, Transformação e Carregamento)**.\n",
        "\n",
        "Para atingir esse objetivo, o foco foi a construção dos códigos em Python e configuração do **Apache Airflow** para automatizar e gerenciar as etapas do ETL.\n",
        "\n",
        "**Os principais passos desta sprint incluiram:**\n",
        "\n",
        "**Desenvolvimento das Rotinas de Extração:** Foi criado o código Python responsável por extrair os dados das fontes de origem. Isso garantiu que os dados fossem coletados de maneira eficiente e confiável.\n",
        "\n",
        "**Transformação dos Dados:** Ocorreram as transformações necessárias nos dados extraídos. Isso permitiu incluir limpeza, formatação, agregação e enriquecimento dos dados para análise.\n",
        "\n",
        "**Carregamento no Data Warehouse:** Após a transformação, os dados foram carregados no Data Warehouse, onde foram armazenados de forma estruturada e organizada. **O Apache Airflow** foi configurado para agendar e executar essas tarefas de carregamento de maneira automatizada e programada.\n",
        "\n",
        "\n",
        "Ao final desta sprint, o processo de ETL estava completamente implementado e funcionando e permitiu que os dados fossem coletados, transformados e carregados no **Data Warehouse** de forma automatizada e confiável."
      ],
      "metadata": {
        "id": "bQVxJN92XFJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**01- Construindo os códigos ETL com a Linguagem Python**\n",
        "\n",
        "**Objetivo:**\\\n",
        "Após a modelagem do **Data Warehouse**, a criação das tabelas de dimensões e da tabela Fato, foi construído o processo de **ETL - Extração, Transformação e Carregamento com a linguagem Python e o Apache Airflow.**\n"
      ],
      "metadata": {
        "id": "h9LrqIpT95fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1- Primeiro Bloco de Código \" O processo de Imports\"**"
      ],
      "metadata": {
        "id": "3WiraH3hXX0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro Bloco de comandos os \"Imports\"\n",
        "import csv\n",
        "import airflow\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.postgres_operator import PostgresOperator\n",
        "from airflow.utils.dates import days_ago"
      ],
      "metadata": {
        "id": "2DZbC8rH-A9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comentando os código do Pacote imports**\n",
        "\n",
        "* **import \"CSV\"** : Importa o módulo \"csv\" em Python. Nossa fonte de dados é um arquivo CSV. Portanto, foi necessário manipular esses arquivos em algum momento.\n",
        "\n",
        "* **import \"airflow\"** : Importa o pacote \"airflow\" em Python pois foi necessário construir toda a **DAG (Directed Acyclic Graph, ou Grafo Direcionado Acíclico)**, que foi lida pelo Apache Airflow.\n",
        "\n",
        "* **import \"time\"** : Importa o módulo **\"time\"** em Python. fornece funcionalidades relacionadas ao tempo e à temporização.\n",
        "\n",
        "* **import pandas as pd** : Importa o módulo **\"pandas\"** em Python que foi renomeado para **\"pd\"** para facilitar referências posteriores. O **\"pandas\"** é uma biblioteca amplamente usada para manipulação e análise de dados. Forneceu estruturas de dados e funções para trabalhar com dados tabulares.\n",
        "\n",
        "**Em seguida, temos os imports específicos do Apache Airflow:**\n",
        "\n",
        "* **from datetime import datetime e from datetime import timedelta:**\n",
        " Importa as classes **datetime e timedelta** do módulo **datetime** para lidar com datas e horários.\n",
        "\n",
        "* **from airflow import DAG:** Importa a classe **DAG** do pacote **airflow**, que é fundamental para definir e configurar fluxos de trabalho no Apache Airflow.\n",
        "\n",
        "* **from airflow.operators.python_operator import PythonOperator:** Importa a classe **PythonOperator** do **pacote airflow.operators.python_operator**, que permite executar tarefas Python como parte de uma DAG.\n",
        "\n",
        "* **from airflow.operators.postgres_operator import PostgresOperator:** Importa a classe **PostgresOperator do pacote airflow.operators.postgres_operator**, que é usada para executar tarefas relacionadas ao PostgreSQL em uma DAG.\n",
        "\n",
        "* **from airflow.utils.dates import days_ago:** Importa a função **days_ago** do pacote **airflow.utils.dates**, que é útil para calcular datas com base na contagem regressiva de dias a partir da data atual.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "nXYPbpQsNf8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2- Segundo Bloco de Código \" Criação da DAG - Directed Acyclic Graphs\"**\n",
        "###**Tópico Argumentos**"
      ],
      "metadata": {
        "id": "cxB4AXDMW_Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Argumentos\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n"
      ],
      "metadata": {
        "id": "VRDvChFVX_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Argumentos**\n",
        "\n",
        "**Argumentos:** representa um direcionário de argumentos\n",
        "\n",
        "* **'owner': 'airflow'** : Indica que o próprietario ou responsável pela DAG é o Airflow.\n",
        "\n",
        "* **start_date': datetime(2023, 1, 1)** : Indica que a data e a hora que a DAG começará a ser agendada e executada. A DAG começará a ser executada a partir do primeiro dia de janeiro de 2023. A partir desse ponto, a DAG será agendada de acordo com o cronograma especificado, que pode ser definido usando as outras configurações, como **schedule_interval.**\n",
        "\n",
        " A data  foi retroativa **datetime(2023, 1, 1)**, pois quando criamos uma GAG, nos não conseguimos mais modificar esta data, dessa forma é sempre bom colocarmos uma data retroativa, para que comecemos a execuação no momento que começarmos a DAG.\n",
        "\n",
        "* **'depends_on_past': False'** : Indica que a execução atual da DAG não depende do sucesso da execução anterior. Ou seja, cada execução da DAG ocorrerá independentemente do resultado das execuções anteriores.\n",
        "\n",
        "* **'retries': 1** : Indica que em caso de falha na execução da tarefa, haverá 01 tentativa de retomar a tarefa. Se a tarefa falhar novamente após essa tentativa, não haverá mais retentativas, e a execução será considerada como falha permanente.\n",
        "\n",
        "* **'retry_delay': timedelta(minutes=5)** : Indica que o intervalo de tempo que deve ser aguardado antes de tentar novamente uma tarefa que falhou. Neste caso, após uma falha, a tarefa será reagendada para uma nova tentativa após 5 minutos. Isso permite um atraso entre as tentativas para evitar sobrecarregar o sistema em caso de falhas frequentes.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rbOskjGpYLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tópico criação da DAG**"
      ],
      "metadata": {
        "id": "WDy7N2Gc65Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a DAG\n",
        "\n",
        "# https://crontab.guru/\n",
        "dag_log_solutions = DAG(dag_id = \"logsol\",\n",
        "                   default_args = default_args,\n",
        "                   schedule_interval = '0 0 * * *',\n",
        "                   dagrun_timeout = timedelta(minutes = 60),\n",
        "                   description = 'Job ETL de Carga no DW com Airflow',\n",
        "                   start_date = airflow.utils.dates.days_ago(1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "VYx3GUsRdLhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Criar DAG**\n",
        "\n",
        "\n",
        "* **dag_log_solutions = DAG(dag_id = \"logsol\"** : Indica que a DAG está sendo nomeada como \"dag_log_solutions_dsa\" com o ID \"logsol\".\n",
        "\n",
        "* **default_args = default_args** : Indica a Definição doo argumento e o conteúdo desse argumento que foi a **lista de agumentos definida na etapa anterior.**\n",
        "\n",
        "* **schedule_interval = '0 0 * * *'** : Indica a Definição do intervalo de agendamento: No caso de \"schedule_interval = '0 0 * * *'\", isso significa o seguinte:\n",
        "\n",
        " \"0 0 * * *\"é uma expressão cron que define a programação. Ela indica que a DAG será executada diariamente à meia-noite (00:00) todos os dias.\n",
        "Portanto, a DAG será agendada para ser executada uma vez por dia, sempre à meia-noite, de acordo com esse cronograma.\n",
        "\n",
        "* **dagrun_timeout = timedelta(minutes = 60)** : Indica a Definição do tempo máximo permitido para a execução de uma instância da DAG (DAG run). Isso significa que cada execução da DAG tem um limite de tempo de 60 minutos (1 hora). Se a execução da DAG não for concluída dentro desse limite de tempo, ela será considerada falha.\n",
        "\n",
        "* **description = 'Job ETL de Carga no DW com Airflow'** : Indica a Definição de uma descrição ou um resumo do que é o trabalho (job) realizado pela DAG.\n",
        "\n",
        "* **start_date = airflow.utils.dates.days_ago(1)** : Indica que o start Date será um dia anterior a data que vamos criar no Airflow, ou seja  indica que a DAG começará a ser executada um dia atrás da data atual.\n",
        "Essa configuração permite agendar a DAG para ser executada a partir de um ponto no passado, o que pode ser útil em cenários em que você deseja retroativamente executar tarefas ou processar dados a partir de uma data anterior.\n",
        "\n",
        "**Obs:** Temos também um start_date no  default_args, irá gerar conflito? Não gerará conflito e se tivermos o argumentos em dois locais, vale por ultimo o que estiver a DAG\n",
        "\n",
        "**Dessa forma temos a DAG criada que é o bloco principal para execuação do Apahe Airflow.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wxP55sglkQUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3- Terceiro Bloco de Código \" Função para Carregar dados no DW**"
      ],
      "metadata": {
        "id": "1BSxGQR08YTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_carrega_dados_clientes(**kwargs):\n",
        "\n",
        "    # Get the csv file path\n",
        "    csv_file_path = kwargs['params']['csv_file_path']\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n",
        "\n",
        "            # Consulta SQL com placeholders\n",
        "            sql_query_cli = f\"INSERT INTO varejo.DIM_CLIENTE ({columns}) VALUES ({placeholders})\"\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_CLIENTE.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bkYufMXVXBnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "N13Wf3a1Q6nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.4- Comentando os tópicos da Função**\n",
        "\n",
        "Tópico: def func_carrega_dados_clientes(**kwargs):\n",
        "\n",
        "* **def** : Esta é uma palavra-chave Python usada para definir uma função. É o começo da definição da função.\n",
        "\n",
        "* **func_carrega_dados_clientes** : Este é o nome da função. É o identificador pelo qual você pode chamar a função posteriormente no seu programa. O nome da função é escolhido pelo programador e deve seguir as regras de nomenclatura do Python.\n",
        "\n",
        "* (**kwargs)** : Esta é uma parte importante da definição da função e se refere aos parâmetros da função. Vou dividir isso em partes menores:\n",
        "\n",
        "* **( )** : Os parênteses vazios indicam que a função não aceita parâmetros posicionais, ou seja, não requer argumentos posicionais quando você a chama.\n",
        "\n",
        "* ****kwargs* : Este é um parâmetro especial que permite que a função aceite um número variável de argumentos nomeados. Aqui estão os detalhes:\n",
        "\n",
        "****** : É um operador de descompactação que permite que você passe um dicionário de argumentos nomeados para a função.\n",
        "\n",
        "**kwargs** : É um nome comum usado para se referir aos argumentos nomeados passados para a função. Esses argumentos são empacotados em um dicionário onde as chaves são os nomes dos argumentos e os valores são os valores associados.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uheSjRlUX0W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Get the csv file path ( Obter o caminho do Arquivo CSV)**"
      ],
      "metadata": {
        "id": "WL8-DvIWSyyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the csv file path\n",
        "csv_file_path = kwargs['params']['csv_file_path']"
      ],
      "metadata": {
        "id": "MPOfRxPhe3Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **csv_file_path** : Esta é uma variável que está sendo definida. Ela irá armazenar o valor associado à chave **'csv_file_path'** dentro do dicionário **kwargs**. O nome da variável, neste caso, sugere que ela provavelmente armazenará um caminho de arquivo para um arquivo CSV.\n",
        "\n",
        "* **kwargs** : É uma convenção comum para nomear o dicionário de argumentos nomeados passados para uma função. Dentro desse dicionário, você pode ter várias chaves e valores que foram passados para a função. Ele permite que você acesse esses valores com base em suas chaves.\n",
        "\n",
        "* **['params']** : Isso indica que você está acessando um valor específico dentro do dicionário **kwargs** usando a chave **'params'**. Presumivelmente, dentro do dicionário **kwargs**, há uma chave **'params'** que está associada a outro dicionário.\n",
        "\n",
        "* **['csv_file_path']** : Agora você está acessando uma chave específica dentro do dicionário associado à chave **'params'**. Isso significa que há um dicionário aninhado dentro do dicionário **kwargs** sob a chave **'params'**, e dentro deste dicionário aninhado, você está procurando o valor associado à chave **'csv_file_path'**.\n",
        "\n",
        "Em resumo, essa linha de código está extraindo o valor associado à chave **'csv_file_path'** de um dicionário aninhado dentro do dicionário **kwargs** e atribuindo esse valor à variável **csv_file_path**. Essa variável provavelmente será usada como o caminho do arquivo para um arquivo CSV em operações posteriores no código.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FW20RRZBdR91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Inicializar o contador**"
      ],
      "metadata": {
        "id": "cU0TcmmjejYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o contador\n",
        "    i = 0"
      ],
      "metadata": {
        "id": "9CxxHIIqevMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nesse código, a linha i = 0 está realizando a seguinte ação:**\n",
        "\n",
        "* **i** : Isso é uma variável. Em programação, uma variável é usada para armazenar valores, como números, texto ou outros tipos de dados.\n",
        "\n",
        "* **= 0** : Isso é um operador de atribuição. Ele atribui o valor à direita (neste caso, 0) à variável à esquerda **(neste caso, i)**. Em outras palavras, a variável **i** está sendo inicializada com o **valor 0**.\n",
        "\n",
        "Portanto, essa linha de código está iniciando a variável **i** e atribuindo o valor **0** a ela. Isso é comum em programação, especialmente em loops e contadores, onde você deseja começar com um valor específico e, em seguida, incrementá-lo ou modificá-lo à medida que o programa é executado. O valor **0** é frequentemente usado como um ponto de partida padrão para muitos contadores em linguagens de programação.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "acMYbOKIe-R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Abrir o aqruivo CSV**"
      ],
      "metadata": {
        "id": "CqMJzUUMl7jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the csv file\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:"
      ],
      "metadata": {
        "id": "NhbfsRDSmH6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **with open(csv_file_path, 'r') as f** : Esta linha abre o arquivo CSV especificado em csv_file_path no modo de leitura ('r'). A cláusula with é usada para garantir que o arquivo seja corretamente fechado após seu uso, mesmo se ocorrerem exceções durante o processo.\n",
        "\n",
        "* **reader = csv.DictReader(f)** : Aqui, é criado um objeto reader usando o módulo csv.DictReader. Este objeto é usado para ler o conteúdo do arquivo CSV. O DictReader trata a primeira linha do arquivo CSV como cabeçalho, o que significa que as colunas se tornarão chaves em um dicionário.\n",
        "\n",
        "* **for item in reader** : Este é um loop for que itera por cada linha do arquivo CSV. Em cada iteração, a variável item contém uma linha de dados do arquivo. A linha é representada como um dicionário, onde as chaves são os nomes das colunas (a partir do cabeçalho) e os valores são os dados nas respectivas colunas.\n",
        "\n",
        "Com esse código, você pode percorrer todas as linhas do arquivo CSV e acessar os dados de cada linha como um dicionário, o que facilita muito o processamento e a manipulação dos dados.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YvxTWcldmMY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Extrair uma linha como dicionário**"
      ],
      "metadata": {
        "id": "Yrae3GE3rV3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)"
      ],
      "metadata": {
        "id": "D2m_Kt27rgpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **dados_cli = dict(item)** : Esta linha cria um novo dicionário chamado **dados_cli**. O dicionário é construído a partir do dicionário **item**. A função **dict()** é usada para criar uma cópia do dicionário item. Isso é feito para que possamos acessar facilmente os valores dos campos na linha atual do **arquivo CSV** usando as chaves correspondentes.\n",
        "\n",
        "No geral, essa linha cria uma cópia dos dados da linha atual do CSV em um dicionário chamado **dados_cli**, permitindo um acesso mais fácil e organizado aos valores dos campos dessa linha.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4GkWigC6rkb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Definir as colunas e placeholders para a consulta SQL**"
      ],
      "metadata": {
        "id": "VuXe8uH9tGKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define as colunas e placeholders para a consulta SQL\n",
        "            columns = ', '.join(dados_cli.keys())\n",
        "            placeholders = ', '.join(['%s'] * len(dados_cli))\n"
      ],
      "metadata": {
        "id": "msJmURm0xpBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **columns = ', '.join(dados_cli.keys())** : Nesta linha, estamos construindo a **string columns**, que conterá uma lista de nomes de colunas separados por vírgulas. **O método dados_cli.keys()** retorna uma lista das chaves **(nomes de colunas)** no **dicionário dados_cli**. A função **join()** é usada para concatenar essas chaves **(nomes de colunas)** em uma única **string**, separando-as por vírgulas. O resultado será algo como **\"Nome, Idade, Cidade\"**, onde as vírgulas separam os nomes das colunas.\n",
        "\n",
        "* **placeholders = ', '.join(['%s'] * len(dados_cli))** : Nesta linha, estamos construindo a string placeholders, que conterá uma lista de marcadores de posição **(\"%s\")** separados por vírgulas. A quantidade de marcadores de posição é determinada pelo número de elementos no **dicionário dados_cli.** Para fazer isso, primeiro criamos uma lista contendo **%s** repetido o número de vezes igual ao tamanho do dicionário **dados_cli** usando a lista de compreensão. Em seguida, usamos a função **join()** para concatenar esses marcadores de posição em uma única string, separados por vírgulas.\n",
        "\n",
        "No geral, essas linhas são frequentemente usadas em consultas SQL para inserção de dados em um banco de dados. A primeira linha **(columns)** cria a parte da consulta SQL que lista os nomes das colunas nas quais os dados serão inseridos, enquanto a segunda linha **(placeholders)** cria a parte da consulta que contém marcadores de posição para os valores que serão inseridos nessas colunas. A combinação dessas duas strings pode ser usada em uma consulta SQL para inserção de dados em um banco de dados.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "25H6PfGyxtFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Consultar SQL com placeholders**"
      ],
      "metadata": {
        "id": "3Nf98lXIzSX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_query_cli = f\"INSERT INTO varejo.DIM_CLIENTE ({columns}) VALUES ({placeholders})\""
      ],
      "metadata": {
        "id": "CLZYDrdBzbvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trecho de código, estamos construindo uma consulta SQL para inserir dados em uma tabela chamada \"varejo.DIM_CLIENTE\". Vou explicar o que cada elemento faz:\n",
        "\n",
        "* **sql_query_cli** : Esta é uma variável que armazenará a consulta SQL completa que será usada para inserir os dados na tabela.\n",
        "\n",
        "* **f\"INSERT INTO varejo.DIM_CLIENTE ({columns}) VALUES ({placeholders})\"** : Esta é uma **f-string**, uma forma de criar strings formatadas em Python. Aqui está o que cada parte faz:\n",
        "\n",
        "**\"INSERT INTO varejo.DIM_CLIENTE\"** : Esta é a parte da consulta SQL que indica que estamos realizando uma operação de inserção de dados na tabela **\"varejo.DIM_CLIENTE\"**.\n",
        "\n",
        "**({columns})** : Isso é uma substituição, onde **{columns}** será substituído pela string que foi construída anteriormente e contém os nomes das colunas na tabela. Por exemplo, se columns for **\"Nome, Idade, Cidade\"**, a consulta SQL ficará assim: **\"INSERT INTO varejo.DIM_CLIENTE (Nome, Idade, Cidade)\"**.\n",
        "\n",
        "**\"VALUES\"** : Esta parte da consulta SQL indica que estamos fornecendo os valores que serão inseridos na tabela.\n",
        "\n",
        "**({placeholders})** : Isso é outra substituição, onde {placeholders} será substituído pela string que contém marcadores de posição para os valores que serão inseridos. Por exemplo, se placeholders **for \"%s, %s, %s\"** (com base na quantidade de colunas e valores), a consulta SQL ficará assim: **\"VALUES (%s, %s, %s)\"**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lKQsu0A7zg2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Operador do Postgres com incremento no id da tarefa (para cada linha inserida)**"
      ],
      "metadata": {
        "id": "lB-IF5Nx5PH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 parameters = list(dados_cli.values()),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "QMv42OJj5dMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trecho de código, foi configurando um **operador PostgreSQL** no Apache Airflow para executar uma consulta SQL e inserir dados em uma tabela. Vou explicar o que cada elemento faz:\n",
        "\n",
        "* **postgres_operator** : Esta é uma variável que representa o operador PostgreSQL que será configurado.\n",
        "\n",
        "* **PostgresOperator** : Isso cria uma instância do operador PostgreSQL no Apache Airflow.\n",
        "\n",
        "* **task_id** : É um identificador único para a tarefa dentro do fluxo (DAG). O identificador é gerado dinamicamente com base no valor de i. Isso é feito para que cada linha de dados seja inserida em uma tarefa separada, com um identificador único.\n",
        "\n",
        "* **sql** : É a consulta SQL que será executada pelo operador PostgreSQL. No caso, essa consulta é a que construímos anteriormente usando as variáveis columns e placeholders.\n",
        "\n",
        "* **parameters** : Esta é uma lista de parâmetros que serão usados para preencher os marcadores de posição na consulta SQL. Os valores da lista são extraídos do dicionário dados_cli usando o método .values(). Isso garante que os valores sejam inseridos nas colunas correspondentes de acordo com a consulta SQL.\n",
        "\n",
        "* **postgres_conn_id** : Este é o identificador da conexão PostgreSQL que o Apache Airflow usará para se conectar ao banco de dados. A conexão com o nome 'LOGDW' deve ser configurada antecipadamente no Airflow e conter as informações de conexão necessárias, como host, porta, nome do banco de dados, usuário e senha.\n",
        "\n",
        "* **dag** : É o DAG (Directed Acyclic Graph) ao qual esta tarefa pertence. O operador é adicionado a este DAG para que o Apache Airflow saiba como organizar e executar as tarefas em um fluxo.\n",
        "\n",
        "Portanto, esse trecho de código configura um **operador PostgreSQL** que executa a consulta **SQL sql_query_cli** com os parâmetros parameters no banco de dados configurado com a conexão **'LOGDW'**.\n",
        "\n",
        "Cada vez que esse operador é executado **(por exemplo, durante a execução do DAG)**, ele insere uma linha de dados na tabela do banco de dados. **O task_id** é incrementado para que cada linha de dados seja inserida em uma tarefa separada com um nome exclusivo, como **'carrega_dados_clientes_0'**, **'carrega_dados_clientes_1'** e assim por diante.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "0yr_f5hd5e8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Executar o Operador**"
      ],
      "metadata": {
        "id": "bdtBwZ72-T7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executa o operador\n",
        "            postgres_operator.execute(context = kwargs)"
      ],
      "metadata": {
        "id": "f6TmKiP6AH7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trecho de código, estamos chamando o método **execute do operador PostgreSQL configurado anteriormente**.\n",
        "\n",
        "* **postgres_operator** : Este é o operador PostgreSQL que configuramos anteriormente. O método execute será chamado nesse operador para executar a tarefa.\n",
        "\n",
        "* **.execute(context=kwargs)**: O método execute é chamado com um único argumento chamado context, que é igual a **kwargs**.\n",
        "\n",
        "* **context** : O contexto é um dicionário que contém informações e variáveis relacionadas à execução da tarefa. É uma prática comum no Apache Airflow passar o contexto para as tarefas para que elas possam acessar informações sobre a execução atual, como a data de execução, parâmetros, variáveis definidas e muito mais.\n",
        "\n",
        "* **kwargs** : kwargs é uma abreviação de **\"keyword arguments\"** e é frequentemente usado para passar argumentos nomeados em Python. Nesse contexto, kwargs representa o dicionário de contexto que contém informações relevantes para a execução da tarefa.\n",
        "\n",
        "Portanto, o código **postgres_operator.execute(context=kwargs)** está iniciando a execução do **operador PostgreSQL** com base no contexto fornecido em **kwargs**. Isso é importante porque o contexto pode conter informações necessárias para a execução da consulta SQL, como os parâmetros e outras variáveis relevantes para a tarefa.\n",
        "\n",
        "Em resumo, este código inicia a execução da tarefa do **operador PostgreSQL**, usando as informações no contexto **kwargs** para preencher os detalhes da execução da tarefa, como a data de execução, parâmetros e outros dados relevantes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nOcylLkzAIrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: tarefa_carrega_dados_clientes = PythonOperator**"
      ],
      "metadata": {
        "id": "K-FkTyqkBUBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_CLIENTE.csv'}},\n",
        "        dag = dag_log_solutions\n",
        "    )\n"
      ],
      "metadata": {
        "id": "U_NJpG3GBa3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código criou uma tarefa no **Apache Airflow** usando o **operador PythonOperator** para executar uma **função Python**.\n",
        "\n",
        "* **tarefa_carrega_dados_clientes** : Este é o nome da tarefa. É uma instância do **operador PythonOperator** que será adicionada ao **DAG (grafo acíclico direcionado)** chamado **dag_log_solutions**.\n",
        "\n",
        "* **task_id** : Define o identificador exclusivo para essa tarefa no contexto do DAG. Neste caso, o identificador é **'tarefa_carrega_dados_clientes'**, que é o nome da tarefa.\n",
        "\n",
        "* **python_callable** : Isso define a função Python que será executada quando a tarefa for acionada. No código, ele se refere à função **func_carrega_dados_clientes**, que contém a lógica que você deseja executar. Essa função será chamada quando a tarefa for executada.\n",
        "\n",
        "* **provide_context = True** : Isso configura a tarefa para passar o contexto do Airflow para a **função func_carrega_dados_clientes**. O contexto contém informações sobre a execução da tarefa, como a data da execução, parâmetros, variáveis definidas e muito mais. Ao definir provide_context como True, a **função func_carrega_dados_clientes** pode acessar esse contexto.\n",
        "\n",
        "* **op_kwargs** : Este é um dicionário que permite passar parâmetros para a função func_carrega_dados_clientes. No código, você está passando um parâmetro chamado 'params' que é outro dicionário. Esse dicionário contém o caminho do arquivo CSV que a função usará **('csv_file_path')**.\n",
        "\n",
        "* **dag**: Isso especifica o **DAG (grafo acíclico direcionado)** ao qual a tarefa pertence. No caso, a tarefa faz parte do DAG chamado 'dag_log_solutions'.\n",
        "\n",
        "Portanto, este código criou uma tarefa chamada **'tarefa_carrega_dados_clientes'** que executará a função **func_carrega_dados_clientes** e fornecerá o contexto do Airflow para essa função. A função receberá parâmetros definidos em **op_kwargs**, como o caminho do **arquivo CSV**. Quando o DAG for executado, esta tarefa será acionada, o que executará a **função func_carrega_dados_clientes** com os parâmetros e o contexto apropriados.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eRIJqrLFBfKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.5- Observação Importante**:\n",
        "Os dados que vão compor a tabela **Fatos** e as dimensções abaixo\n",
        "* **Transportadoras**;\n",
        "* **Depósitos**;\n",
        "* **Entregas**;\n",
        "* **Frete**;\n",
        "* **Pagamentos**\n",
        "* **Data**\n",
        "\n",
        "Passaram pelo mesmo processo acima de criação dos codigos, ou seja o mesmo processo de criação do processo ETL - Extração, Transformação e carregamento desses dados que foi submedido os dados que vão compor a tebala cliente do DW, foram aplicados aos dados das demais dimensões e Fato que serão carregados no DW.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oE98pwxbreUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: limpar as tabelas Antes do Carregamento dos Dados**\n",
        "\n",
        "Os códigos abaixo foram construidos para executar operações de limpeza em tabelas específicas antes do carregamento de novos dados nessas tabelas do DW.\n",
        "\n",
        "Essa é uma prática comum em processos de **ETL (Extração, Transformação e Carregamento)** para garantir que as tabelas de destino estejam vazias ou em um estado limpo antes de receberem novos dados. **Isso ajuda a evitar conflitos ou duplicatas durante o carregamento de dados.**"
      ],
      "metadata": {
        "id": "kkzHmaT2vL3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limpando a Tabela Fato**"
      ],
      "metadata": {
        "id": "aOZmoBPUJWqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_tb_fato = PostgresOperator(task_id = 'tarefa_trunca_tb_fato', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.TB_FATO CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "NJgkUzZXChxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **tarefa_trunca_tb_fato**: Isso é uma variável que representa a tarefa que está sendo definida. O nome **tarefa_trunca_tb_fato** é um identificador único para essa tarefa dentro do **DAG (Directed Acyclic Graph)**. Você usará esse identificador para referenciar e controlar essa tarefa em outras partes do código.\n",
        "\n",
        "* **task_id** : É um parâmetro obrigatório e é o identificador único da tarefa dentro do DAG. No exemplo, o task_id é definido como 'tarefa_trunca_tb_fato', o que significa que esta tarefa será conhecida pelo nome 'tarefa_trunca_tb_fato'.\n",
        "\n",
        "* **postgres_conn_id** : É o identificador da conexão com o banco de dados PostgreSQL que será usada para executar a consulta SQL. Essa conexão deve estar previamente configurada no Airflow. No exemplo, o postgres_conn_id é definido como 'LOGDW', que é o nome da conexão configurada.\n",
        "\n",
        "* **sql** : É o SQL que será executado pelo operador. Neste caso, a consulta SQL é **\"TRUNCATE TABLE varejo.TB_FATO CASCADE\",** o que significa que a tarefa executará uma operação TRUNCATE na tabela TB_FATO do esquema varejo.\n",
        "\n",
        "* **CASCADE** :  indica que qualquer dependência da tabela também será truncada.\n",
        "\n",
        "* **dag** : É o DAG ao qual esta tarefa pertence. Um DAG é um fluxo de trabalho composto por tarefas interconectadas. A tarefa que está sendo definida aqui faz parte do DAG chamado dag_log_solutions.\n",
        "\n",
        "Em resumo, o código cria uma tarefa chamada **'tarefa_trunca_tb_fato'** que executa uma **consulta SQL** de truncamento em uma tabela específica em um banco de dados **PostgreSQL**. Essa tarefa pode ser adicionada a um fluxo de trabalho (DAG) e programada para ser executada conforme necessário dentro do fluxo de trabalho.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yOEsLiCZJdoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando as demais Tabelas**\n",
        "\n",
        "O mesmo processo que realizamos acima foi replicado para demais tabelas do DW antes de receberem os dados."
      ],
      "metadata": {
        "id": "Ggx-hkkyKpSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_CLIENTE**"
      ],
      "metadata": {
        "id": "WdJTOXnqK1PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_cliente = PostgresOperator(task_id = 'tarefa_trunca_dim_cliente', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_CLIENTE CASCADE\", dag = dag_log_solutions)\n"
      ],
      "metadata": {
        "id": "MuviIIClLDNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_PAGAMENTO**"
      ],
      "metadata": {
        "id": "6xTOysQNLZYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_cliente = PostgresOperator(task_id = 'tarefa_trunca_dim_cliente', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_CLIENTE CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "ldxmytFTLole"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_FRETE**"
      ],
      "metadata": {
        "id": "5BV5cJT1L5UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_frete = PostgresOperator(task_id = 'tarefa_trunca_dim_frete', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_FRETE CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "8Za5DsX0MBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_DATA**"
      ],
      "metadata": {
        "id": "D8tgbgZoMY6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_data = PostgresOperator(task_id = 'tarefa_trunca_dim_data', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_DATA CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "tAm0h15HMhLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_TRANSPORTADORA**"
      ],
      "metadata": {
        "id": "2q8b59CPMpCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_transportadora = PostgresOperator(task_id = 'tarefa_trunca_dim_transportadora', postgres_conn_id = 'LOGDW'\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_TRANSPORTADORA CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "ohCny3vyMv0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_ENTREGA**"
      ],
      "metadata": {
        "id": "50fYUwJ_M7Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_entrega = PostgresOperator(task_id = 'tarefa_trunca_dim_entrega', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_ENTREGA CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "cnh9ah-ENDis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Limpando a tabela DIM_DEPOSITO**"
      ],
      "metadata": {
        "id": "1xInFYPKNJJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_dim_deposito = PostgresOperator(task_id = 'tarefa_trunca_dim_deposito', postgres_conn_id = 'LOGDW',\n",
        "sql = \"TRUNCATE TABLE varejo.DIM_DEPOSITO CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "zZLFrDtgNTKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------"
      ],
      "metadata": {
        "id": "sVNOC8bsNacC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico criando o UpStream:**\n",
        "\n",
        "**\"Upstream\"** se refere às tarefas que precedem outras tarefas no fluxo de execução.\n",
        "\n",
        "O codigo abaixo é uma estrutura de dependência entre tarefas que é usada para garantir que as tarefas sejam executadas na ordem correta, com base nas dependências definidas.\n",
        "\n",
        "Portanto, as tarefas **upstream** devem ser concluídas antes que as tarefas **downstream** possam ser iniciadas. É uma maneira de controlar a execução sequencial de tarefas em um fluxo de trabalho."
      ],
      "metadata": {
        "id": "PACFAk9rOaGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **tarefa_trunca_tb_fato >>**: Esta parte indica que a tarefa chamada \"tarefa_trunca_tb_fato\" é uma tarefa upstream, ou seja, é uma tarefa que deve ser executada antes das tarefas que a seguem no fluxo.\n",
        "\n",
        "* **tarefa_trunca_dim_cliente >>**: Isso significa que a tarefa \"tarefa_trunca_dim_cliente\" é outra tarefa upstream que deve ser executada após a conclusão da tarefa \"tarefa_trunca_tb_fato\".\n",
        "\n",
        "**O mesmo padrão é repetido para as demais tarefas, onde cada uma delas depende da conclusão das tarefas upstream anteriores.**"
      ],
      "metadata": {
        "id": "QzejGZdkQqrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upstream\n",
        "\n",
        "tarefa_trunca_tb_fato >> tarefa_trunca_dim_cliente >>\n",
        "\n",
        "tarefa_trunca_dim_pagamento >> tarefa_trunca_dim_frete >>\n",
        "\n",
        "tarefa_trunca_dim_data >> tarefa_trunca_dim_transportadora >>\n",
        "\n",
        "tarefa_trunca_dim_entrega >> tarefa_trunca_dim_deposito >>\n",
        "\n",
        "tarefa_carrega_dados_clientes >> tarefa_carrega_dados_transportadora >>\n",
        "\n",
        "tarefa_carrega_dados_deposito >> tarefa_carrega_dados_entrega >>\n",
        "\n",
        "tarefa_carrega_dados_frete >> tarefa_carrega_dados_pagamento >>\n",
        "\n",
        "tarefa_carrega_dados_data >> tarefa_carrega_dados_fatos\n"
      ],
      "metadata": {
        "id": "dENTpT27PF9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4J1KKRKSPvbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.6- Tópico: Bloco main**\n",
        "\n",
        "é usado para executar um bloco de código apenas se o arquivo estiver sendo executado como um programa principal. O código dentro do bloco if só será executado se o arquivo for executado diretamente da linha de comando ou se for chamado pelo interpretador Python.\n",
        "\n",
        "Se o arquivo for importado como um módulo, o código dentro do bloco if não será executado."
      ],
      "metadata": {
        "id": "6HEyqx52P27u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco main\n",
        "if __name__ == \"__main__\":\n",
        "    dag_log_solutions.cli()\n"
      ],
      "metadata": {
        "id": "S762oNcmcRDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **if name == \"main\"** : Esta instrução condicional verifica se o arquivo está sendo executado como um programa principal. Se for, o código dentro do bloco if será executado.\n",
        "\n",
        "* **dag_log_solutions.cli()** : Esta função chama a função cli() no módulo dag_log_solutions. **A função cli()** é responsável por executar a interface de linha de comando do aplicativo."
      ],
      "metadata": {
        "id": "Xnb6RZLocWYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "xxhvUjPn4wC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Sprint 02 - Finalizada**\n",
        "\n",
        "O processo de ETL estava completo, implementado e funcionando. Isso permitiu que os dados fossem coletados, transformados e carregados no Data Warehouse de forma automatizada e confiável."
      ],
      "metadata": {
        "id": "aXvNkuq_c_RR"
      }
    }
  ]
}